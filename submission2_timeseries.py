# -*- coding: utf-8 -*-
"""Submission2_TimeSeries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dh5KG9LVUloRi8JsTGgvl6PRbbFuiYLa

# Import Dataset from Kaggle
"""

!kaggle datasets download -d aayushkandpal/air-quality-time-series-data-uci

import zipfile
zip_ref = zipfile.ZipFile('/content/air-quality-time-series-data-uci.zip', 'r')
zip_ref.extractall('tmp')
zip_ref.close()

import pandas as pd
df = pd.read_excel('/content/tmp/AirQualityUCI.xlsx')
df.head()

df.info()

df_baru = df[['Date', 'RH']]
df_baru = df_baru.dropna()
df_baru.head()

df_baru.isnull().sum()

"""# Visual Inspection"""

import matplotlib.pyplot as plt

date = df_baru['Date'].values
rh  = df_baru['RH'].values
 
plt.figure(figsize=(15,5))
plt.plot(date, rh)
plt.title('Relative Humidity',
          fontsize=18);

"""# Prepare Data"""

df_baru = df_baru.astype({"RH": float})

rh

rh1 = rh.reshape(-1,1)
rh1

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

min_max_scaler = StandardScaler()
rh = min_max_scaler.fit_transform(rh1)
rh

"""# Training and Testing """

from sklearn.model_selection import train_test_split

rh_latih, rh_test = train_test_split(rh, test_size=0.2, shuffle=False)
print(rh_latih.shape, rh_test.shape)

import tensorflow as tf

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(rh_latih, window_size=60, batch_size=128, shuffle_buffer=1000)
test_set = windowed_dataset(rh_test, window_size=60, batch_size=128, shuffle_buffer=1000)

"""# Create Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense
from tensorflow.keras.callbacks import EarlyStopping

mycallback = EarlyStopping(monitor="val_loss", mode="min", patience=2)

model = Sequential()
model.add(LSTM(64, return_sequences=True))
model.add(LSTM(64))
model.add(Dense(30, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='relu'))
model.add(Dense(1))

model.compile(optimizer=tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9),
              loss=tf.keras.losses.Huber(), metrics=['mae'])

history = model.fit(train_set, validation_data=(test_set), 
                    epochs=100, callbacks=mycallback)

"""# Plotting MAE and loss"""

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('model mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""# Threshold MAE"""

threshold_mae = (rh.max() - rh.min()) * 10/100
print(threshold_mae)